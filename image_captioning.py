# -*- coding: utf-8 -*-
"""Image Captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tiBzcpWr9pbxdzbHUZxFMREmyciJnckT
"""

!pip install tensorflow

"""Import the necessary modules"""

import os
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from PIL import Image
from math import ceil
from collections import defaultdict
from tqdm.notebook import tqdm # progress bar

"""Import deep learning frameworks"""

import tensorflow as tf

# image feature extraction
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# captions tokenization
from tensorflow.keras.preprocessing.text import Tokenizer

# padding sequences to a specific length
from tensorflow.keras.preprocessing.sequence import pad_sequences

# models
from tensorflow.keras.models import Model, load_model

# layers
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, LayerNormalization, MultiHeadAttention, concatenate

# utils
from tensorflow.keras.utils import plot_model, to_categorical

# checking the score
from nltk.translate.bleu_score import corpus_bleu

# input and output directories
INPUT_DIR = '/content/drive/MyDrive/Vision-Language-Model'
OUTPUT_DIR = '/content/drive/MyDrive/Vision-Language-Model/Output'

if not os.path.exists(INPUT_DIR):
  raise FileNotFoundError("Input directory not found")

if not os.path.exists(OUTPUT_DIR):
  os.makedirs(OUTPUT_DIR)

"""We are going to use pretrained vgg model. Remove the output layers(classification task) to get our output features and use those features to fine-tune our model."""

inputs_shape = (224, 224, 3) # input shape expected by VGG16
inputs = Input(shape=inputs_shape)
base_model = VGG16(include_top=True, weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)
print(model.summary())

# store image features
image_features = {} # key: imageID without extension, value: feature vector of image

img_dir = os.path.join(INPUT_DIR, 'Images')

# loop through each image
for img_name in tqdm(os.listdir(img_dir)):
  # load image
  img_path = os.path.join(img_dir, img_name)
  img = load_img(img_path, target_size=(224, 224))
  # convert the image pixels to numpy array
  img = img_to_array(img)
  # reshape the image data for the model
  img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))
  # preprocess image
  img = preprocess_input(img)
  img_feature = model.predict(img, verbose=0) # get feature vector of the image(value)
  img_id = img_name.split('.')[0] # get the image ID by removing the extension in image(key)
  image_features[img_id] = img_feature

print(len(image_features))
image_features['1000268201_693b08cb0e']
print(image_features)

# store image features in pickle
pickle.dump(image_features, open(os.path.join(OUTPUT_DIR, 'img_features.pkl'), 'wb'))

# Load features from pickle file
pickle_file_path = os.path.join(OUTPUT_DIR, 'img_features.pkl')
with open(pickle_file_path, 'rb') as file:
    loaded_features = pickle.load(file)

with open(os.path.join(INPUT_DIR, 'captions.txt'), 'r') as f:
  next(f)
  captions_doc = f.read()

image_to_caption_mapping = defaultdict(list)

# process lines from captions.txt
for line in tqdm(captions_doc.split('\n')):
  # split by commas(,)
  tokens = line.split(',')
  if len(tokens) < 2:
    continue
  image_id, *captions = tokens # split the tokens into image_id and captions
  image_id = image_id.split('.')[0] # remove the extensions
  caption = " ".join(captions)
  image_to_caption_mapping[image_id].append(caption)

total_captions = sum(len(captions) for captions in image_to_caption_mapping.values())
print(f'Total number of captions: {total_captions}')

# before preprocess
image_to_caption_mapping['1000268201_693b08cb0e']

# preprocess captions
def preprocess_captions(mapping):
  for key, captions in tqdm(mapping.items()):
    for i in range(len(captions)):
      # one caption at a time
      caption = captions[i]
      # convert to lower case
      caption = caption.lower()
      # delete digits and other special characters
      caption = ''.join(char for char in caption if char.isalnum() or char.isspace())
      # remove extra spaces
      caption = caption.replace('\s+', ' ')
      # add start and end tags
      caption = 'startseq ' + ' '.join([word for word in caption.split() if len(word) > 1]) + ' endseq'
      captions[i] = caption

# after preprocess
preprocess_captions(image_to_caption_mapping)
image_to_caption_mapping['1000268201_693b08cb0e']

# create a list of all captions
all_captions = [caption for captions in image_to_caption_mapping.values() for caption in captions]

all_captions[:10]

# tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
print(f'Vocabulary size: {vocab_size}')

# save the tokenizer
with open(os.path.join(OUTPUT_DIR, 'tokenizer.pkl'), 'wb') as tokenizer_file:
  pickle.dump(tokenizer, tokenizer_file)

# load the tokenizer
with open(os.path.join(OUTPUT_DIR, 'tokenizer.pkl'), 'rb') as tokenizer_file:
  tokenizer = pickle.load(tokenizer_file)

vocab_size = len(tokenizer.word_index) + 1
print(f'Vocabulary size: {vocab_size}')

# Calculate maximum caption length
max_caption_length = max(len(tokenizer.texts_to_sequences([caption])[0]) for caption in all_captions)
print(f'Maximum caption length: {max_caption_length}')

"""Creating train test split"""

# list of image ID's
image_ids = list(image_to_caption_mapping.keys())
# split into train and test sets
train_size = int(0.9 * len(image_ids))
train = image_ids[:train_size]
test = image_ids[train_size:]

print(len(train), len(test))

def prepare_data(image_id, image_to_caption_mapping, features, tokenizer, max_caption_length, vocab_size):
    """
    Prepare input-output pairs for a given image and its captions.
    """
    X1, X2, y = [], [], []
    captions = image_to_caption_mapping[image_id]
    for caption in captions:
        # Convert the caption to a sequence of token IDs
        caption_seq = tokenizer.texts_to_sequences([caption])[0]
        for i in range(1, len(caption_seq)):
            # Input sequence (padded) and one-hot encoded output
            in_seq = pad_sequences([caption_seq[:i]], maxlen=max_caption_length)[0]
            out_seq = to_categorical([caption_seq[i]], num_classes=vocab_size)[0]

            # Append flattened features to match expected model input shape
            X1.append(features[image_id].flatten())  # Ensure shape is (4096,)
            X2.append(in_seq)
            y.append(out_seq)
    return X1, X2, y


def data_generator(image_ids, image_to_caption_mapping, features, tokenizer, max_caption_length, vocab_size, batch_size):
    """
    Generator function to yield batches of input-output pairs.
    """
    X1_buffer, X2_buffer, y_buffer = [], [], []
    while True:
        for image_id in image_ids:
            X1, X2, y = prepare_data(image_id, image_to_caption_mapping, features, tokenizer, max_caption_length, vocab_size)
            X1_buffer.extend(X1)
            X2_buffer.extend(X2)
            y_buffer.extend(y)

            # Yield batches when the buffer reaches batch size
            while len(X1_buffer) >= batch_size:
                yield (
                    (np.array(X1_buffer[:batch_size]), np.array(X2_buffer[:batch_size])),  # Input tuple (X1, X2)
                    np.array(y_buffer[:batch_size]),  # Output y
                )
                # Remove used data from buffers
                X1_buffer = X1_buffer[batch_size:]
                X2_buffer = X2_buffer[batch_size:]
                y_buffer = y_buffer[batch_size:]

from tensorflow.keras.layers import RepeatVector, Bidirectional, Lambda
from tensorflow.keras.layers import Dot, Activation

# Encoder
# image feature layers
inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
fe1_projected = RepeatVector(max_caption_length)(fe2)
fe2_projected = Bidirectional(LSTM(256, return_sequences=True))(fe1_projected)

# sequence feature layers
inputs2 = Input(shape=(max_caption_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)

# apply attention mechanism
attention = Dot(axes=(2, 2))([fe2_projected, se3])  # compute attention scores
attention_scores = Activation('softmax')(attention)

# Apply attention scores to sequence embeddings
attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se2])

# Sum the attended sequence embeddings along the time axis
context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attention_context)

# Decoder model
decoder_input = concatenate([context_vector, fe2], axis=-1)
decoder1 = Dense(256, activation='relu')(decoder_input)
outputs = Dense(vocab_size, activation='softmax')(decoder1)

# Create the model
model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# Visualize the model
plot_model(model, show_shapes=True)

# Set the number of epochs, batch size
epochs = 100
batch_size = 32

# Calculate the steps_per_epoch based on the number of batches in one epoch
steps_per_epoch = ceil(len(train) / batch_size)
validation_steps = ceil(len(test) / batch_size)  # Calculate the steps for validation data

# Loop through the epochs for training
for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")

    # Set up data generators
    train_generator = data_generator(train, image_to_caption_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)
    test_generator = data_generator(test, image_to_caption_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)

    model.fit(train_generator, epochs=1, steps_per_epoch=steps_per_epoch,
          validation_data=test_generator, validation_steps=validation_steps,
          verbose=1)

model.save(OUTPUT_DIR+'/lstm_model.h5')

def get_word_from_index(index, tokenizer):
    return next((word for word, idx in tokenizer.word_index.items() if idx == index), None)

def predict_caption(model, image_features, tokenizer, max_caption_length):
    # Initialize the caption sequence
    caption = 'startseq'

    # Generate the caption
    for _ in range(max_caption_length):
        # Convert the current caption to a sequence of token indices
        sequence = tokenizer.texts_to_sequences([caption])[0]
        # Pad the sequence to match the maximum caption length
        sequence = pad_sequences([sequence], maxlen=max_caption_length)
        # Predict the next word's probability distribution
        yhat = model.predict([image_features, sequence], verbose=0)
        # Get the index with the highest probability
        predicted_index = np.argmax(yhat)
        # Convert the index to a word
        predicted_word = get_word_from_index(predicted_index, tokenizer)

        # Append the predicted word to the caption
        caption += " " + predicted_word

        # Stop if the word is None or if the end sequence tag is encountered
        if predicted_word is None or predicted_word == 'endseq':
            break

    return caption

